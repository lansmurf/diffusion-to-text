{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CODEFUSION(nn.Module):\n",
    "    def __init__(self, num_heads, num_layers_denoiser, num_layers_decoder, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Load the pre-trained CodeT5 model\n",
    "        self.encoder = T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-small')\n",
    "        encoder_config = self.encoder.config\n",
    "\n",
    "        # Set vocab_size and in_channels from the pre-trained model's configuration\n",
    "        vocab_size = encoder_config.vocab_size\n",
    "        in_channels = encoder_config.d_model\n",
    "\n",
    "        # Embedding layer L for code snippets\n",
    "        self.code_embedding = nn.Embedding(vocab_size, in_channels)\n",
    "\n",
    "        # Define model_channels based on CodeT5's configuration\n",
    "        model_channels = encoder_config.d_model\n",
    "        \n",
    "        # Time embedding layer dimensions\n",
    "        time_embed_dim = model_channels * 4\n",
    "        self.time_embed = nn.Sequential(\n",
    "            nn.Linear(model_channels, time_embed_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_embed_dim, in_channels),  # Aligning with CodeT5's hidden size\n",
    "        )\n",
    "        \n",
    "        # Denoiser (N) using TransformerBlocks\n",
    "        self.denoiser = nn.ModuleList([\n",
    "            TransformerBlock(model_channels, num_heads, dropout=dropout) for _ in range(num_layers_denoiser)\n",
    "        ])\n",
    "\n",
    "        # Decoder (D) using TransformerBlocks\n",
    "        self.decoder = nn.ModuleList([\n",
    "            TransformerBlock(model_channels, num_heads, dropout=dropout) for _ in range(num_layers_decoder)\n",
    "        ])\n",
    "\n",
    "        # Classification Head (H) for token prediction\n",
    "        self.classification_head = nn.Linear(model_channels, vocab_size)\n",
    "\n",
    "        # Ensure weight sharing between the embedding and the classification head\n",
    "        self.classification_head.weight = self.code_embedding.weight\n",
    "\n",
    "    def forward(self, code, Es, timesteps, scaled_gaussian_noise, pretrain_mode=True):\n",
    "        \n",
    "        emb_code = self.code_embedding(code)\n",
    "        emb_timestep = self.time_embed(timestep_embedding(timesteps, 512))\n",
    "        seq_length = emb_code.size(1)\n",
    "        noisy_emb_code = emb_code * scaled_gaussian_noise\n",
    "        emb_inputs = noisy_emb_code + emb_timestep.unsqueeze(1).expand(-1, seq_length, -1)\n",
    "        \n",
    "        if pretrain_mode:\n",
    "            encoded_utterances = torch.randn_like(emb_inputs)\n",
    "            \n",
    "        # Encoding natural language utterances\n",
    "        else:\n",
    "            encoder_outputs = self.encoder(input_ids=utterance)\n",
    "            encoded_utterances = encoder_outputs.last_hidden_state\n",
    "\n",
    "        # Denoising code embeddings\n",
    "        denoised_embeddings = emb_inputs\n",
    "        \n",
    "        for layer in self.denoiser:\n",
    "            denoised_embeddings = layer(denoised_embeddings, context=encoded_utterances)\n",
    "\n",
    "        # Decoding to get final hidden representations\n",
    "        decoded_embeddings = denoised_embeddings\n",
    "        for layer in self.decoder:\n",
    "            decoded_embeddings = layer(decoded_embeddings, context=encoded_utterances)\n",
    "        \n",
    "        if pretrain_mode:\n",
    "            predicted_noise = noisy_emb_code - denoised_embedding\n",
    "            return predicted_noise, decoded_embeddings, emb_code\n",
    "        # Predicting code tokens\n",
    "        logits = self.classification_head(decoded_embeddings)\n",
    "        return logits\n",
    "\n",
    "    def get_embeds(self, input_ids):\n",
    "        return self.code_embedding(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, num_attention_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(dim, num_attention_heads, dropout=dropout)\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.cross_attn = nn.MultiheadAttention(dim, num_attention_heads, dropout=dropout)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(dim, dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(dim * 4, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.norm3 = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x, context=None):\n",
    "        # Self-attention\n",
    "        x = x + self.self_attn(x, x, x)[0]\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # Cross-attention with the encoded utterance, if provided\n",
    "        if context is not None:\n",
    "            x = x + self.cross_attn(x, context, context)[0]\n",
    "            x = self.norm2(x)\n",
    "\n",
    "        # Feed-forward\n",
    "        x = x + self.feed_forward(x)\n",
    "        x = self.norm3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def timestep_embedding(timesteps, dim, max_period=10000):\n",
    "    half_dim = dim // 2\n",
    "    emb = math.log(max_period) / (half_dim - 1)\n",
    "    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n",
    "    emb = emb.to(timesteps.device)\n",
    "    emb = timesteps[:, None].float() * emb[None, :]\n",
    "    emb = torch.cat((torch.sin(emb), torch.cos(emb)), dim=1)\n",
    "    if dim % 2:\n",
    "        emb = torch.cat((emb, torch.zeros_like(emb[:, :1])), dim=1)\n",
    "    return emb\n",
    "\n",
    "class Denoiser(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_layers, dropout=0.1):\n",
    "        super(Denoiser, self).__init__()\n",
    "        self. denoiser = nn.ModuleList([\n",
    "            TransformerBlock(d_model, nhead, dropout=dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        time_embed_dim = d_model * 4\n",
    "        self.time_embed = nn.Sequential(\n",
    "            nn.Linear(d_model, time_embed_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_embed_dim, d_model),\n",
    "        )\n",
    "\n",
    "    def forward(self, emb_code, Es, timesteps, pretrain_mode=True):\n",
    "        \n",
    "        emb = self.time_embed(timestep_embedding(timesteps, 512))\n",
    "        seq_length = emb_code.size(1)\n",
    "        emb_inputs = emb_code + emb.unsqueeze(1).expand(-1, seq_length, -1)\n",
    "        \n",
    "        if pretrain_mode:\n",
    "            encoded_utterances = torch.randn_like(emb_inputs)\n",
    "            \n",
    "        # Encoding natural language utterances\n",
    "        else:\n",
    "            encoder_outputs = self.encoder(input_ids=utterance)\n",
    "            encoded_utterances = encoder_outputs.last_hidden_state\n",
    "\n",
    "        # Denoising code embeddings\n",
    "        denoised_embeddings = emb_inputs\n",
    "        \n",
    "        for layer in self.denoiser:\n",
    "            denoised_embeddings = layer(denoised_embeddings, context=encoded_utterances)\n",
    "        \n",
    "        return denoised_embeddings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "484af2d38e80b0ae"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def mean_flat(tensor):\n",
    "    \"\"\"\n",
    "    Take the mean over all non-batch dimensions.\n",
    "    \"\"\"\n",
    "    return tensor.mean(dim=list(range(1, len(tensor.shape))))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "20d5e9688870caa8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Toy dataset of Python code snippets\n",
    "code_snippets = [\n",
    "    \"for i in range(5): print(i)\",\n",
    "    \"import numpy as np\",\n",
    "    \"x = np.array([1, 2, 3])\",\n",
    "    \"def add(a, b): return a + b\",\n",
    "    \"print(add(5, 3))\"\n",
    "]\n",
    "\n",
    "# Hyperparameters\n",
    "embed_dim = 512\n",
    "seq_len = max(len(snippet) for snippet in code_snippets)\n",
    "batch_size = 5\n",
    "\n",
    "# Simulate embeddings for code snippets and encoded utterances\n",
    "original_embeddings = torch.randn(batch_size, seq_len, embed_dim)\n",
    "E_s = torch.randn(batch_size, seq_len, embed_dim)  # Simulated encoded utterances (flattened for simplicity)\n",
    "\n",
    "# Denoiser model\n",
    "denoiser = Denoiser(embed_dim, num_layers=10, nhead=8)\n",
    "optimizer = optim.AdamW(denoiser.parameters(), lr=5e-4)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "max_epochs = 100\n",
    "max_timestep = 1200\n",
    "noise_schedule = sqrt_noise_scheduler(max_timestep)\n",
    "#uniform_sampler = UniformSampler(denoiser)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Generate random timesteps for each example in the batch\n",
    "    timesteps = torch.randint(0, max_timestep, (batch_size,))\n",
    "    #print(timesteps.shape)\n",
    "    # Index into the precomputed noise schedule using the generated timesteps\n",
    "    noise_levels = noise_schedule[timesteps].view(-1, 1, 1)\n",
    "    \n",
    "    # Generate Gaussian noise scaled by the noise levels\n",
    "    scaled_gaussian_noise = torch.randn_like(original_embeddings) * noise_levels\n",
    "    \n",
    "    # Add scaled Gaussian noise to the original embeddings to create noisy embeddings\n",
    "    noisy_embeddings_with_scaled_noise = original_embeddings + scaled_gaussian_noise\n",
    "    \n",
    "    # Use the Gaussian noise as E_s_noisy for the denoiser\n",
    "    E_s_noisy = scaled_gaussian_noise  # Optional: Use scaled noise as part of E_s_noisy if needed\n",
    "\n",
    "    #print(timesteps.shape)\n",
    "    #print(noisy_embeddings_with_scaled_noise.shape)\n",
    "    # Forward pass with noisy embeddings\n",
    "    denoised_embedding = denoiser(noisy_embeddings_with_scaled_noise, E_s, timesteps)\n",
    "\n",
    "    predicted_noise = noisy_embeddings_with_scaled_noise - denoised_embedding\n",
    "    # Calculate loss between predicted noise and the actual (scaled) noise used\n",
    "    noise_prediction_loss = (predicted_noise - scaled_gaussian_noise) ** 2\n",
    "    #print(noise_prediction_loss.shape)\n",
    "    loss = mean_flat(noise_prediction_loss).mean()\n",
    "    #print(loss)\n",
    "\n",
    "    # Backward pass and optimization step\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch}, Loss: {loss.item()}, Noise Level: {noise_levels.mean().item()}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a58050cd8905f470"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
